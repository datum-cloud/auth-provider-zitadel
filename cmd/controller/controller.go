/*
Copyright 2025.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package controller

import (
	"context"
	"crypto/tls"
	"errors"
	"flag"
	"fmt"
	"os"
	"path/filepath"

	// Import all Kubernetes client auth plugins (e.g. Azure, GCP, OIDC, etc.)
	// to ensure that exec-entrypoint and run can make use of them.

	"github.com/go-logr/logr"
	"github.com/spf13/cobra"
	"github.com/zitadel/oidc/v3/pkg/client/profile"
	"go.miloapis.com/auth-provider-zitadel/internal/config"
	"golang.org/x/sync/errgroup"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/runtime/serializer"
	utilruntime "k8s.io/apimachinery/pkg/util/runtime"
	clientgoscheme "k8s.io/client-go/kubernetes/scheme"
	_ "k8s.io/client-go/plugin/pkg/client/auth"
	"k8s.io/client-go/rest"
	"k8s.io/client-go/tools/clientcmd"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/certwatcher"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/cluster"
	"sigs.k8s.io/controller-runtime/pkg/healthz"
	"sigs.k8s.io/controller-runtime/pkg/log/zap"
	"sigs.k8s.io/controller-runtime/pkg/manager"
	"sigs.k8s.io/controller-runtime/pkg/metrics/filters"
	metricsserver "sigs.k8s.io/controller-runtime/pkg/metrics/server"
	"sigs.k8s.io/controller-runtime/pkg/webhook"
	mcmanager "sigs.k8s.io/multicluster-runtime/pkg/manager"
	"sigs.k8s.io/multicluster-runtime/pkg/multicluster"
	mcsingle "sigs.k8s.io/multicluster-runtime/providers/single"

	"go.miloapis.com/auth-provider-zitadel/internal/controller"
	zitadelHtppClient "go.miloapis.com/auth-provider-zitadel/internal/zitadel"
	iammiloapiscomv1alpha1 "go.miloapis.com/milo/pkg/apis/iam/v1alpha1"

	milomulticluster "go.miloapis.com/milo/pkg/multicluster-runtime"
	miloprovider "go.miloapis.com/milo/pkg/multicluster-runtime/milo"
	// +kubebuilder:scaffold:imports
)

var (
	scheme   = runtime.NewScheme()
	setupLog = ctrl.Log.WithName("setup")
	codecs   = serializer.NewCodecFactory(scheme, serializer.EnableStrict)
)

func init() {
	utilruntime.Must(clientgoscheme.AddToScheme(scheme))

	utilruntime.Must(iammiloapiscomv1alpha1.AddToScheme(scheme))
	// +kubebuilder:scaffold:scheme
}

// nolint:gocyclo
func NewControllerCommand(globalConfig *config.GlobalConfig) *cobra.Command {
	cfg := config.NewControllerConfig()

	cmd := &cobra.Command{
		Use:   "controller",
		Short: "Run the Kubernetes controller manager",
		Long: `Run the Kubernetes controller manager that watches for custom resources
and manages the auth provider lifecycle.`,
		RunE: func(cmd *cobra.Command, args []string) error {
			return runController(cfg, globalConfig)
		},
	}

	// Zitadel flags
	cmd.Flags().StringVar(&cfg.Zitadel.BaseURL, "zitadel-base-url", "", "The base URL of the Zitadel instance.")
	cmd.Flags().StringVar(&cfg.Zitadel.MachineAccountKeyPath, "zitadel-machine-account-key-path", "", "The path to the machine account key file generated by Zitadel.")

	// Metrics flags
	cmd.Flags().StringVar(&cfg.Metrics.Addr, "metrics-bind-address", "0", "The address the metrics endpoint binds to. "+"Use :8443 for HTTPS or :8080 for HTTP, or leave as 0 to disable the metrics service.")
	cmd.Flags().StringVar(&cfg.Metrics.CertPath, "metrics-cert-path", "", "The directory that contains the metrics server certificate.")
	cmd.Flags().StringVar(&cfg.Metrics.CertName, "metrics-cert-name", "tls.crt", "The name of the metrics server certificate file.")
	cmd.Flags().StringVar(&cfg.Metrics.CertKey, "metrics-cert-key", "tls.key", "The name of the metrics server key file.")
	cmd.Flags().BoolVar(&cfg.Metrics.SecureMetrics, "metrics-secure", true, "If set, the metrics endpoint is served securely via HTTPS. Use --metrics-secure=false to use HTTP instead.")

	// Webhook flags
	cmd.Flags().StringVar(&cfg.Webhook.CertPath, "webhook-cert-path", "", "The directory that contains the webhook certificate.")
	cmd.Flags().StringVar(&cfg.Webhook.CertName, "webhook-cert-name", "tls.crt", "The name of the webhook certificate file.")
	cmd.Flags().StringVar(&cfg.Webhook.CertKey, "webhook-cert-key", "tls.key", "The name of the webhook key file.")

	// Config flags
	cmd.Flags().BoolVar(&cfg.EnableHTTP2, "enable-http2", false, "If set, HTTP/2 will be enabled for the metrics and webhook servers")
	cmd.Flags().StringVar(&cfg.UpstreamClusterKubeconfig, "upstream-kubeconfig", "", "Path to the kubeconfig file for the upstream cluster. If empty, uses the default kubeconfig.")
	cmd.Flags().StringVar(&cfg.CoreControlPlaneKubeconfig, "core-control-plane-kubeconfig", "", "Path to the kubeconfig file for the core control plane cluster. If empty, uses the default kubeconfig.")
	cmd.Flags().StringVar(&cfg.ServerConfigFile, "server-config", "", "path to the server config file")
	cmd.Flags().StringVar(&cfg.EmailAddressSuffix, "email-address-suffix", "iam.miloapis.com", "The suffix of the email address for the machine account.")
	cmd.Flags().StringVar(&cfg.ProbeAddr, "health-probe-bind-address", ":8081", "The address the probe endpoint binds to.")

	// Leader election flags
	cmd.Flags().BoolVar(&cfg.LeaderElection.Enabled, "leader-elect", cfg.LeaderElection.Enabled, "Enable leader election for controller manager. Enabling this will ensure there is only one active controller manager.")
	cmd.Flags().StringVar(&cfg.LeaderElection.ID, "leader-election-id", cfg.LeaderElection.ID, "The name of the resource object that is used for locking during leader election.")
	cmd.Flags().StringVar(&cfg.LeaderElection.Namespace, "leader-election-namespace", cfg.LeaderElection.Namespace, "The namespace in which the leader election resource will be created. If empty, uses the current namespace.")
	cmd.Flags().StringVar(&cfg.LeaderElection.ResourceLock, "leader-election-resource-lock", cfg.LeaderElection.ResourceLock, "The type of resource object that is used for locking during leader election. Supported options are 'leases', 'endpointsleases' and 'configmapsleases'.")
	cmd.Flags().DurationVar(&cfg.LeaderElection.LeaseDuration, "leader-election-lease-duration", cfg.LeaderElection.LeaseDuration, "The duration that non-leader candidates will wait after observing a leadership renewal until attempting to acquire leadership of a led but unrenewed leader slot.")
	cmd.Flags().DurationVar(&cfg.LeaderElection.RenewDeadline, "leader-election-renew-deadline", cfg.LeaderElection.RenewDeadline, "The interval between attempts by the acting master to renew a leadership slot before it stops leading.")
	cmd.Flags().DurationVar(&cfg.LeaderElection.RetryPeriod, "leader-election-retry-period", cfg.LeaderElection.RetryPeriod, "The duration the clients should wait between attempting acquisition and renewal of a leadership.")
	cmd.Flags().BoolVar(&cfg.LeaderElection.ReleaseOnCancel, "leader-election-release-on-cancel", cfg.LeaderElection.ReleaseOnCancel, "If the leader should step down voluntarily when the Manager ends. This requires the binary to immediately end when the Manager is stopped.")

	opts := zap.Options{
		Development: true,
	}

	opts.BindFlags(flag.CommandLine)
	flag.Parse()

	return cmd
}

// nolint:gocyclo
func runController(cfg *config.ControllerConfig, globalConfig *config.GlobalConfig) error {
	setupLog.Info("Machine Account Email address suffix", "emailAddressSuffix", cfg.EmailAddressSuffix)

	// Log leader election configuration
	if cfg.LeaderElection.Enabled {
		setupLog.Info("Leader election enabled",
			"id", cfg.LeaderElection.ID,
			"namespace", cfg.LeaderElection.Namespace,
			"resource-lock", cfg.LeaderElection.ResourceLock,
			"lease-duration", cfg.LeaderElection.LeaseDuration,
			"renew-deadline", cfg.LeaderElection.RenewDeadline,
			"retry-period", cfg.LeaderElection.RetryPeriod,
			"release-on-cancel", cfg.LeaderElection.ReleaseOnCancel,
		)
	} else {
		setupLog.Info("Leader election disabled")
	}

	// Ensure Zitadel connection information is provided.
	if cfg.Zitadel.BaseURL == "" {
		setupLog.Error(fmt.Errorf("zitadel-base-url must be specified"), "")
		os.Exit(1)
	}

	if cfg.Zitadel.MachineAccountKeyPath == "" {
		setupLog.Error(fmt.Errorf("zitadel-machine-account-key-path must be specified"), "")
		os.Exit(1)
	}

	if len(cfg.ServerConfigFile) == 0 {
		setupLog.Error(fmt.Errorf("must provide --server-config"), "")
		os.Exit(1)
	}

	var serverConfig config.AuthProviderZitadel
	data, err := os.ReadFile(cfg.ServerConfigFile)
	if err != nil {
		setupLog.Error(fmt.Errorf("unable to read server config from %q", cfg.ServerConfigFile), "")
		os.Exit(1)
	}

	if err := runtime.DecodeInto(codecs.UniversalDecoder(), data, &serverConfig); err != nil {
		setupLog.Error(err, "unable to decode server config")
		os.Exit(1)
	}

	setupLog.Info("server config", "config", serverConfig)

	// if the enable-http2 flag is false (the default), http/2 should be disabled
	// due to its vulnerabilities. More specifically, disabling http/2 will
	// prevent from being vulnerable to the HTTP/2 Stream Cancellation and
	// Rapid Reset CVEs. For more information see:
	// - https://github.com/advisories/GHSA-qppj-fm5r-hxr3
	// - https://github.com/advisories/GHSA-4374-p667-p6c8
	disableHTTP2 := func(c *tls.Config) {
		setupLog.Info("disabling http/2")
		c.NextProtos = []string{"http/1.1"}
	}

	if !cfg.EnableHTTP2 {
		cfg.TlsOpts = append(cfg.TlsOpts, disableHTTP2)
	}

	// Create watchers for metrics and webhooks certificates
	var metricsCertWatcher, webhookCertWatcher *certwatcher.CertWatcher

	// Initial webhook TLS options
	webhookTLSOpts := cfg.TlsOpts

	if len(cfg.Webhook.CertPath) > 0 {
		setupLog.Info("Initializing webhook certificate watcher using provided certificates",
			"webhook-cert-path", cfg.Webhook.CertPath, "webhook-cert-name", cfg.Webhook.CertName, "webhook-cert-key", cfg.Webhook.CertKey)

		var err error
		webhookCertWatcher, err = certwatcher.New(
			filepath.Join(cfg.Webhook.CertPath, cfg.Webhook.CertName),
			filepath.Join(cfg.Webhook.CertPath, cfg.Webhook.CertKey),
		)
		if err != nil {
			setupLog.Error(err, "Failed to initialize webhook certificate watcher")
			os.Exit(1)
		}

		webhookTLSOpts = append(webhookTLSOpts, func(config *tls.Config) {
			config.GetCertificate = webhookCertWatcher.GetCertificate
		})
	}

	webhookServer := webhook.NewServer(webhook.Options{
		TLSOpts: webhookTLSOpts,
	})

	// Metrics endpoint is enabled in 'config/default/kustomization.yaml'. The Metrics options configure the server.
	// More info:
	// - https://pkg.go.dev/sigs.k8s.io/controller-runtime@v0.20.2/pkg/metrics/server
	// - https://book.kubebuilder.io/reference/metrics.html
	metricsServerOptions := metricsserver.Options{
		BindAddress:   cfg.Metrics.Addr,
		SecureServing: cfg.Metrics.SecureMetrics,
		TLSOpts:       cfg.TlsOpts,
	}

	if cfg.Metrics.SecureMetrics {
		// FilterProvider is used to protect the metrics endpoint with authn/authz.
		// These configurations ensure that only authorized users and service accounts
		// can access the metrics endpoint. The RBAC are configured in 'config/rbac/kustomization.yaml'. More info:
		// https://pkg.go.dev/sigs.k8s.io/controller-runtime@v0.20.2/pkg/metrics/filters#WithAuthenticationAndAuthorization
		metricsServerOptions.FilterProvider = filters.WithAuthenticationAndAuthorization
	}

	// If the certificate is not specified, controller-runtime will automatically
	// generate self-signed certificates for the metrics server. While convenient for development and testing,
	// this setup is not recommended for production.
	//
	// TODO(user): If you enable certManager, uncomment the following lines:
	// - [METRICS-WITH-CERTS] at config/default/kustomization.yaml to generate and use certificates
	// managed by cert-manager for the metrics server.
	// - [PROMETHEUS-WITH-CERTS] at config/prometheus/kustomization.yaml for TLS certification.
	if len(cfg.Metrics.CertPath) > 0 {
		setupLog.Info("Initializing metrics certificate watcher using provided certificates",
			"metrics-cert-path", cfg.Metrics.CertPath, "metrics-cert-name", cfg.Metrics.CertName, "metrics-cert-key", cfg.Metrics.CertKey)

		var err error
		metricsCertWatcher, err = certwatcher.New(
			filepath.Join(cfg.Metrics.CertPath, cfg.Metrics.CertName),
			filepath.Join(cfg.Metrics.CertPath, cfg.Metrics.CertKey),
		)
		if err != nil {
			setupLog.Error(err, "to initialize metrics certificate watcher", "error", err)
			os.Exit(1)
		}

		metricsServerOptions.TLSOpts = append(metricsServerOptions.TLSOpts, func(config *tls.Config) {
			config.GetCertificate = metricsCertWatcher.GetCertificate
		})
	}

	var upstreamClusterConfig *rest.Config
	if cfg.UpstreamClusterKubeconfig != "" {
		upstreamClusterConfig, err = clientcmd.BuildConfigFromFlags("", cfg.UpstreamClusterKubeconfig)
		if err != nil {
			setupLog.Error(err, "unable to load upstream kubeconfig", "path", cfg.UpstreamClusterKubeconfig)
			os.Exit(1)
		}
	} else {
		upstreamClusterConfig, err = ctrl.GetConfig()
		if err != nil {
			setupLog.Error(err, "unable to load default kubeconfig")
			os.Exit(1)
		}
	}

	deploymentClusterConfig := ctrl.GetConfigOrDie()

	deploymentCluster, err := cluster.New(deploymentClusterConfig, func(o *cluster.Options) {
		o.Scheme = scheme
	})
	if err != nil {
		setupLog.Error(err, "failed to construct downstream cluster")
		os.Exit(1)
	}

	runnables, provider, err := initializeClusterDiscovery(serverConfig, deploymentCluster, scheme)
	if err != nil {
		setupLog.Error(err, "unable to initialize cluster discovery")
		os.Exit(1)
	}

	coreControlPlaneConfig, err := cfg.CoreControlPlaneRestConfig()
	if err != nil {
		setupLog.Error(err, "failed to construct core control plane cluster config")
		os.Exit(1)
	}

	coreControlPlaneCluster, err := cluster.New(coreControlPlaneConfig, func(o *cluster.Options) {
		o.Scheme = scheme
	})
	if err != nil {
		setupLog.Error(err, "failed to construct core control plane cluster")
		os.Exit(1)
	}

	runnables = append(runnables, coreControlPlaneCluster)

	mgrOptions := ctrl.Options{
		Scheme:                 scheme,
		Metrics:                metricsServerOptions,
		WebhookServer:          webhookServer,
		HealthProbeBindAddress: cfg.ProbeAddr,
		LeaderElection:         cfg.LeaderElection.Enabled,
		LeaderElectionID:       "auth-provider-zitadel.datumapis.com",
	}

	// Add optional leader election configuration
	// LeaderElectionReleaseOnCancel defines if the leader should step down voluntarily
	// when the Manager ends. This requires the binary to immediately end when the
	// Manager is stopped, otherwise, this setting is unsafe. Setting this significantly
	// speeds up voluntary leader transitions as the new leader don't have to wait
	// LeaseDuration time first.
	//
	// In the default scaffold provided, the program ends immediately after
	// the manager stops, so would be fine to enable this option. However,
	// if you are doing or is intended to do any operation such as perform cleanups
	// after the manager stops then its usage might be unsafe.
	// LeaderElectionReleaseOnCancel: true,
	if cfg.LeaderElection.Enabled {
		if cfg.LeaderElection.Namespace != "" {
			mgrOptions.LeaderElectionNamespace = cfg.LeaderElection.Namespace
		}
		if cfg.LeaderElection.ResourceLock != "" {
			mgrOptions.LeaderElectionResourceLock = cfg.LeaderElection.ResourceLock
		}
		if cfg.LeaderElection.LeaseDuration > 0 {
			mgrOptions.LeaseDuration = &cfg.LeaderElection.LeaseDuration
		}
		if cfg.LeaderElection.RenewDeadline > 0 {
			mgrOptions.RenewDeadline = &cfg.LeaderElection.RenewDeadline
		}
		if cfg.LeaderElection.RetryPeriod > 0 {
			mgrOptions.RetryPeriod = &cfg.LeaderElection.RetryPeriod
		}
	}

	// Create main multicluster manager with leader election
	mgr, err := mcmanager.New(upstreamClusterConfig, provider, mgrOptions)
	if err != nil {
		setupLog.Error(err, "unable to start multicluster manager")
		os.Exit(1)
	}

	// Create core control plane manager with shared metrics and no leader
	// election.
	coreControlPlaneMgr, err := ctrl.NewManager(coreControlPlaneConfig, ctrl.Options{
		Scheme: scheme,
		Metrics: metricsserver.Options{
			// Disable separate metrics endpoint. The controller runtime library uses
			// a global metrics registry so all metrics will be available in the multi
			// cluster runtime manager's metrics endpoint.
			BindAddress: "0",
		},
		HealthProbeBindAddress: "0",
		// This controller manager will rely on the leadership election of
		// the multi cluster manager to determine if this controller should start.
		LeaderElection: false,
	})
	if err != nil {
		setupLog.Error(err, "unable to create core control plane manager")
		os.Exit(1)
	}

	// Initialise Zitadel client
	tokenSource, err := profile.NewJWTProfileTokenSourceFromKeyFile(
		context.Background(),
		cfg.Zitadel.BaseURL,
		cfg.Zitadel.MachineAccountKeyPath,
		[]string{"urn:zitadel:iam:org:project:id:zitadel:aud"},
	)
	if err != nil {
		setupLog.Error(err, "unable to create token source")
		os.Exit(1)
	}
	zitadelHtppClient := zitadelHtppClient.NewClientWithTokenSource(cfg.Zitadel.BaseURL, tokenSource)

	// Setup MachineAccountController on multicluster manager (for project control planes)
	if err = (&controller.MachineAccountController{
		Zitadel:            zitadelHtppClient,
		EmailAddressSuffix: cfg.EmailAddressSuffix,
	}).SetupWithManager(mgr); err != nil {
		setupLog.Error(err, "unable to create controller", "controller", "MachineAccount")
		os.Exit(1)
	}

	// Setup UserDeactivationController on core control plane manager
	if err = (&controller.UserDeactivationController{
		Client:  coreControlPlaneMgr.GetClient(),
		Zitadel: zitadelHtppClient,
	}).SetupWithManager(coreControlPlaneMgr); err != nil {
		setupLog.Error(err, "unable to create controller", "controller", "UserDeactivation")
		os.Exit(1)
	}

	// Setup UserController on core control plane manager
	if err = (&controller.UserController{
		Client:  coreControlPlaneMgr.GetClient(),
		Zitadel: zitadelHtppClient,
	}).SetupWithManager(coreControlPlaneMgr); err != nil {
		setupLog.Error(err, "unable to create controller", "controller", "User")
		os.Exit(1)
	}

	// Add core control plane manager as a runnable that starts only when main manager is leader
	if err := mgr.Add(&coreControlPlaneRunnable{
		mgr:                 mgr,
		coreControlPlaneMgr: coreControlPlaneMgr,
		setupLog:            setupLog,
	}); err != nil {
		setupLog.Error(err, "unable to set up core control plane manager")
		os.Exit(1)
	}

	if err := mgr.AddHealthzCheck("healthz", healthz.Ping); err != nil {
		setupLog.Error(err, "unable to set up health check")
		os.Exit(1)
	}
	if err := mgr.AddReadyzCheck("readyz", healthz.Ping); err != nil {
		setupLog.Error(err, "unable to set up ready check")
		os.Exit(1)
	}

	ctx := ctrl.SetupSignalHandler()
	g, ctx := errgroup.WithContext(ctx)

	for _, runnable := range runnables {
		g.Go(func() error {
			return ignoreCanceled(runnable.Start(ctx))
		})
	}

	setupLog.Info("starting cluster discovery provider")
	g.Go(func() error {
		return ignoreCanceled(provider.Run(ctx, mgr))
	})

	setupLog.Info("starting multicluster manager")
	g.Go(func() error {
		return ignoreCanceled(mgr.Start(ctx))
	})

	if err := g.Wait(); err != nil {
		setupLog.Error(err, "unable to start")
		os.Exit(1)
	}

	return nil
}

type runnableProvider interface {
	multicluster.Provider
	Run(context.Context, mcmanager.Manager) error
}

// coreControlPlaneRunnable implements multicluster-runtime Runnable interface
type coreControlPlaneRunnable struct {
	mgr                 mcmanager.Manager
	coreControlPlaneMgr manager.Manager
	setupLog            logr.Logger
}

func (r *coreControlPlaneRunnable) Start(ctx context.Context) error {
	<-r.mgr.Elected() // Wait for main manager to become leader
	r.setupLog.Info("Main manager elected leader, starting core control plane manager")
	return r.coreControlPlaneMgr.Start(ctx)
}

func (r *coreControlPlaneRunnable) Engage(ctx context.Context, clusterName string, cluster cluster.Cluster) error {
	// No-op: this runnable doesn't manage clusters
	return nil
}

// Needed until we contribute the patch in the following PR again (need to sign CLA):
//
//	See: https://github.com/kubernetes-sigs/multicluster-runtime/pull/18
type wrappedSingleClusterProvider struct {
	multicluster.Provider
	cluster cluster.Cluster
}

func (p *wrappedSingleClusterProvider) Run(ctx context.Context, mgr mcmanager.Manager) error {
	if err := mgr.Engage(ctx, "single", p.cluster); err != nil {
		return err
	}
	return p.Provider.(runnableProvider).Run(ctx, mgr)
}

func initializeClusterDiscovery(
	serverConfig config.AuthProviderZitadel,
	deploymentCluster cluster.Cluster,
	scheme *runtime.Scheme,
) (runnables []manager.Runnable, provider runnableProvider, err error) {
	runnables = append(runnables, deploymentCluster)
	switch serverConfig.Discovery.Mode {
	case milomulticluster.ProviderSingle:
		provider = &wrappedSingleClusterProvider{
			Provider: mcsingle.New("single", deploymentCluster),
			cluster:  deploymentCluster,
		}

	case milomulticluster.ProviderMilo:
		discoveryRestConfig, err := serverConfig.Discovery.DiscoveryRestConfig()
		if err != nil {
			return nil, nil, fmt.Errorf("unable to get discovery rest config: %w", err)
		}

		projectRestConfig, err := serverConfig.Discovery.ProjectRestConfig()
		if err != nil {
			return nil, nil, fmt.Errorf("unable to get project rest config: %w", err)
		}

		discoveryManager, err := manager.New(discoveryRestConfig, manager.Options{
			Client: client.Options{
				Cache: &client.CacheOptions{
					Unstructured: true,
				},
			},
		})
		if err != nil {
			return nil, nil, fmt.Errorf("unable to set up overall controller manager: %w", err)
		}

		provider, err = miloprovider.New(discoveryManager, miloprovider.Options{
			ClusterOptions: []cluster.Option{
				func(o *cluster.Options) {
					o.Scheme = scheme
				},
			},
			InternalServiceDiscovery: serverConfig.Discovery.InternalServiceDiscovery,
			ProjectRestConfig:        projectRestConfig,
		})
		if err != nil {
			return nil, nil, fmt.Errorf("unable to create datum project provider: %w", err)
		}

		runnables = append(runnables, discoveryManager)

	// case providers.ProviderKind:
	// 	provider = mckind.New(mckind.Options{
	// 		ClusterOptions: []cluster.Option{
	// 			func(o *cluster.Options) {
	// 				o.Scheme = scheme
	// 			},
	// 		},
	// 	})

	default:
		return nil, nil, fmt.Errorf(
			"unsupported cluster discovery mode %s",
			serverConfig.Discovery.Mode,
		)
	}

	return runnables, provider, nil
}

func ignoreCanceled(err error) error {
	if errors.Is(err, context.Canceled) {
		return nil
	}
	return err
}
